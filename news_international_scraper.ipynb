{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc688c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d970f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the the news international XML file\n",
    "URL = 'https://www.thenews.com.pk/rss/1/1'\n",
    "MONGODB_URI = 'mongodb://localhost:27017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45afce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        if(response.status_code == 200):\n",
    "            xml_content = response.text\n",
    "            return xml_content\n",
    "        else:\n",
    "            print(\"could not fetch xml file : \" , response.status_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Could not fetch xml file : ' , e)\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb4963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_publish_date(date):\n",
    "    date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "    parsed_date = datetime.strptime(date , date_format)\n",
    "\n",
    "    return parsed_date\n",
    "\n",
    "def preprocess_description(description):\n",
    "    description = description.strip()\n",
    "    description_cleaned = re.sub(r'&mdash;|<p>|</p>|<p class=\"\">', ' ', description)\n",
    "    \n",
    "    return description_cleaned\n",
    "\n",
    "def preprocess_img_url(img_url):\n",
    "    bs4 = BeautifulSoup(img_url , 'lxml')\n",
    "    image_element = bs4.find('img')\n",
    "    return image_element['src']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a9dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(root):\n",
    "    \n",
    "    news_articles = []\n",
    "        \n",
    "    for item in root.findall('.//item'):\n",
    "        \n",
    "        title = item.find('title').text.strip()\n",
    "        link = item.find('link').text.strip()\n",
    "        description_and_image = item.find('description').text.strip().split(\"\\n\")\n",
    "        image_url = preprocess_img_url(description_and_image[0])\n",
    "\n",
    "        description = preprocess_description(description_and_image[1])\n",
    "        publish_date = preprocess_publish_date(item.find('pubDate').text)\n",
    "\n",
    "            \n",
    "        description_html = BeautifulSoup(description , 'lxml')\n",
    "        description = description_html.get_text().replace('\\n' ,' ')\n",
    "        \n",
    "        news_articles.append({\"title\":title , \n",
    "                              \"link\" :link , \n",
    "                              \"publish_date\":publish_date ,\n",
    "                              \"scraped_date\": datetime.now(),\n",
    "                              \"source\": \"THE NEWS INTERNATIONAL\" , \n",
    "                              \"image_url\" : image_url,\n",
    "                              \"description\":description })\n",
    "        \n",
    "    return news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096e1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This if for filtering out the articles that have been already scraped if they appear again in rss feed\n",
    "\n",
    "def find_disjoint(arr1, arr2):\n",
    "    # Convert arrays to sets\n",
    "    set1 = set(arr1)\n",
    "    set2 = set(arr2)\n",
    "    \n",
    "    # Find disjoint elements\n",
    "    disjoint = set1.difference(set2)\n",
    "    print(\"disjoint : \" , set1.difference(set2) )\n",
    "    #  convert to list\n",
    "    disjoint = list(disjoint)\n",
    "    \n",
    "    return disjoint\n",
    "\n",
    "def filter_articles(articles):\n",
    "    try:\n",
    "        \n",
    "        current_titles = [ article['title'] for article in articles ]\n",
    "        \n",
    "        with open(os.path.abspath('the_news_international_prev_scraped_articles.txt') , 'r') as file:\n",
    "            prev_titles = file.readlines()\n",
    "            prev_titles = [ title.strip() for title in prev_titles ]\n",
    "    \n",
    "        current_titles = find_disjoint(current_titles , prev_titles)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    articles = [ article for article in articles if article[\"title\"] in current_titles ]\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce35eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(page):\n",
    "    \n",
    "    try:\n",
    "        content_area = page.find('div' , class_=\"story-detail\")\n",
    "        content_area_paragraphs = content_area.findAll('p')\n",
    "        content_area_text = [ paragraph.text for paragraph in content_area_paragraphs]\n",
    "        content_area_text = \" \".join(content_area_text)\n",
    "        content_area_text = content_area_text.replace('\\xa0' , \" \")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Unknown Error scraping : \" , e)\n",
    "        return None\n",
    "        \n",
    "    return content_area_text\n",
    "\n",
    "def cache_articles(articles):\n",
    "    \n",
    "    with open(os.path.abspath('the_news_international_prev_scraped_articles.txt') , 'w') as file:\n",
    "        for article in articles:\n",
    "            file.write(article[\"title\"] + '\\n')\n",
    "\n",
    "def scrape_articles(news_articles , old_news_articles):\n",
    "    \n",
    "    for news in news_articles:\n",
    "        url = news[\"link\"]\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        page_scraped = BeautifulSoup(page.text , \"html.parser\")\n",
    "        scraped_content = extract_content(page_scraped)\n",
    "        news[\"content\"] = scraped_content\n",
    "\n",
    "        time.sleep(5)\n",
    "    \n",
    "    cache_articles(old_news_articles)\n",
    "    return news_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ccbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def save_articles(articles):\n",
    "    \n",
    "    client = MongoClient(MONGODB_URI)\n",
    "    \n",
    "    if(len(articles) == 0):\n",
    "        print('No articles to insert')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        database = client.get_database(\"neutra_news\")\n",
    "        news_articles = database.get_collection(\"news_articles\")\n",
    "\n",
    "        result = news_articles.insert_many(articles)\n",
    "\n",
    "        print(\"Articles inserted : \" , len(result.inserted_ids))\n",
    "\n",
    "        client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Unable to find the document due to the following error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5eb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    try:\n",
    "        xml = get_xml(URL)\n",
    "        xml_root = ET.fromstring(xml)\n",
    "\n",
    "        news_articles = extract_xml(xml_root)\n",
    "        latest_news_articles = filter_articles(news_articles)\n",
    "        scraped_news_articles = scrape_articles(latest_news_articles , news_articles)\n",
    "\n",
    "        print(\"prev : \" , len(news_articles))\n",
    "        print(\"new : \" , len(latest_news_articles))\n",
    "        print('Time : ' , datetime.now().strftime(\"%A, %B %d, %Y %I:%M %p\"))\n",
    "    \n",
    "        save_articles(scraped_news_articles)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Unknown Error : \" , e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09adb674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disjoint :  {\"Karachi weather update: City likely to experience 'light to moderate rains'\", 'Court issues non-bailable arrest warrant for KP CM Ali Amin Gandapur', \"'Balochistan attacks': US reaffirms support to Pakistan in fight against terrorism\", 'Pakistani envoy holds strategic talks with Rep Tom Suozzi over call'}\n",
      "https://www.thenews.com.pk/latest/1226971-court-issues-non-bailable-arrest-warrant-for-kp-cm-ali-gandapur\n",
      "https://www.thenews.com.pk/latest/1226948-karachi-likely-to-experience-light-to-moderate-rains-today-pmd\n",
      "https://www.thenews.com.pk/latest/1226941-pakistani-envoy-holds-strategic-talks-with-rep-tom-suozzi-over-call\n",
      "https://www.thenews.com.pk/latest/1226930-balochistan-attacks-us-reaffirms-support-to-pakistan-in-fight-against-terrorism\n",
      "prev :  50\n",
      "new :  4\n",
      "Time :  Wednesday, September 04, 2024 01:36 PM\n",
      "Articles inserted :  4\n"
     ]
    }
   ],
   "source": [
    "scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9716dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
