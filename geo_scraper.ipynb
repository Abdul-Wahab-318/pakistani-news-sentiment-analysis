{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4314c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d07232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the GEO news XML file\n",
    "url = 'https://feeds.feedburner.com/geo/GiKR'\n",
    "MONGODB_URI = 'mongodb://localhost:27017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if(response.status_code == 200):\n",
    "        xml_content = response.text\n",
    "        return xml_content\n",
    "    else:\n",
    "        print(\"could not fetch xml file : \" , response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5e839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_publish_date(date):\n",
    "    date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "    parsed_date = datetime.strptime(date , date_format)\n",
    "\n",
    "    return parsed_date\n",
    "\n",
    "def preprocess_description(description):\n",
    "    description = description.strip()\n",
    "    description_cleaned = re.sub(r'&mdash;|<p>|</p>|<p class=\"\">', ' ', description)\n",
    "    \n",
    "    return description_cleaned\n",
    "\n",
    "def preprocess_img_url(img_url):\n",
    "    bs4 = BeautifulSoup(img_url , 'lxml')\n",
    "    image_element = bs4.find('img')\n",
    "    return image_element['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4046cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(root):\n",
    "    \n",
    "    news_articles = []\n",
    "    for item in root[0].iter('item'):\n",
    "        title = item[0].text.strip()\n",
    "        link = item[1].text\n",
    "        publish_date = preprocess_publish_date(item[2].text)\n",
    "\n",
    "        description_and_image = item[4].text.strip().split(\"\\n\")\n",
    "        image_url = description_and_image[0]\n",
    "        description = preprocess_description(description_and_image[1])\n",
    "        \n",
    "        news_articles.append({\"title\":title , \n",
    "                              \"link\" :link , \n",
    "                              \"image_url\" : image_url ,\n",
    "                              \"publish_date\":publish_date ,\n",
    "                              \"scraped_date\": datetime.now(),\n",
    "                              \"source\": \"GEO\" , \n",
    "                              \"description\":description })\n",
    "        \n",
    "    return news_articles\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a63aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This if for filtering out the articles that have been already scraped if they appear again in rss feed\n",
    "\n",
    "def find_disjoint(arr1, arr2):\n",
    "    # Convert arrays to sets\n",
    "    set1 = set(arr1)\n",
    "    set2 = set(arr2)\n",
    "    \n",
    "    # Find disjoint elements\n",
    "    disjoint = set1.difference(set2)\n",
    "    print(\"disjoint : \" , set1.difference(set2) )\n",
    "    #  convert to list\n",
    "    disjoint = list(disjoint)\n",
    "    \n",
    "    return disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509aafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles(articles):\n",
    "    try:\n",
    "        \n",
    "        current_titles = [ article['title'] for article in articles ]\n",
    "        \n",
    "        with open(os.path.abspath('geo_prev_scraped_articles.txt') , 'r') as file:\n",
    "            prev_titles = file.readlines()\n",
    "            prev_titles = [ title.strip() for title in prev_titles ]\n",
    "    \n",
    "        current_titles = find_disjoint(current_titles , prev_titles)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    articles = [ article for article in articles if article[\"title\"] in current_titles ]\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a97dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(page):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        content_area = page.find('div' , class_=\"content-area\")\n",
    "\n",
    "        if(not content_area):\n",
    "            content_area = page.find('div' , class_=\"long-content\")\n",
    "\n",
    "        content_area_paragraphs = content_area.findAll('p')\n",
    "        content_area_text = [ paragraph.text for paragraph in content_area_paragraphs]\n",
    "        content_area_text = \" \".join(content_area_text)\n",
    "        content_area_text = content_area_text.replace('\\xa0' , \" \")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Unknown Error scraping : \" , e)\n",
    "        return None\n",
    "        \n",
    "    return content_area_text\n",
    "\n",
    "def cache_articles(articles):\n",
    "    \n",
    "    with open(os.path.abspath('geo_prev_scraped_articles.txt') , 'w') as file:\n",
    "        for article in articles:\n",
    "            file.write(article[\"title\"] + '\\n')\n",
    "\n",
    "def scrape_articles(news_articles , old_news_articles):\n",
    "    \n",
    "    for news in news_articles:\n",
    "        url = news[\"link\"]\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        page_scraped = BeautifulSoup(page.text , \"html.parser\")\n",
    "        scraped_content = extract_content(page_scraped)\n",
    "        news[\"content\"] = scraped_content\n",
    "\n",
    "        time.sleep(5)\n",
    "    \n",
    "    cache_articles(old_news_articles)\n",
    "    return news_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a851c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def save_articles(articles):\n",
    "    \n",
    "    client = MongoClient(MONGODB_URI)\n",
    "    \n",
    "    if(len(articles) == 0):\n",
    "        print('No articles to insert')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        database = client.get_database(\"neutra_news\")\n",
    "        news_articles = database.get_collection(\"news_articles\")\n",
    "\n",
    "        result = news_articles.insert_many(articles)\n",
    "\n",
    "        print(\"Articles inserted : \" , len(result.inserted_ids))\n",
    "\n",
    "        client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Unable to find the document due to the following error: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d01ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    try:\n",
    "        xml = get_xml(url)\n",
    "        xml_root = ET.fromstring(xml)\n",
    "\n",
    "        news_articles = extract_xml(xml_root)\n",
    "        latest_news_articles = filter_articles(news_articles)\n",
    "        scraped_news_articles = scrape_articles(latest_news_articles , news_articles)\n",
    "\n",
    "        print(\"prev : \" , len(news_articles))\n",
    "        print(\"new : \" , len(latest_news_articles))\n",
    "        print('Time : ' , datetime.now().strftime(\"%A, %B %d, %Y %I:%M %p\"))\n",
    "    \n",
    "        save_articles(scraped_news_articles)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Unknown Error : \" , e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f275e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disjoint :  {'Light to moderate isolated rains may lash parts of Karachi today', 'Did CJP postpone meeting of JC in anticipation of judicial package for his extension?', 'Envoy Rizwan Saeed, Congressman Tom Suozzi discuss Pak-US bilateral relations', \"US stands 'shoulder-to-shoulder' with Pakistan in fight against terrorism: Miller\", 'Non-bailable arrest warrants issued against KP CM Ali Amin Gandapur'}\n",
      "https://www.geo.tv/latest/562543-non-bailable-arrest-warrants-issued-against-kp-cm-ali-amin-gandapur\n",
      "https://www.geo.tv/latest/562498-light-to-moderate-isolated-rains-may-lash-parts-of-karachi-today\n",
      "https://www.geo.tv/latest/562495-legal-expert-weighs-in-on-rumours-about-postponement-of-jcp-meeting-by-cjp-isa\n",
      "https://www.geo.tv/latest/562487-amb-rizwan-saeed-rep-tom-suozzi-discuss-pak-us-bilateral-relations\n",
      "https://www.geo.tv/latest/562484-balochistan-attacks-us-reaffirms-support-to-pakistan-in-fight-against-terrorism\n",
      "prev :  20\n",
      "new :  5\n",
      "Time :  Wednesday, September 04, 2024 01:36 PM\n",
      "Articles inserted :  5\n"
     ]
    }
   ],
   "source": [
    "scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every(1).hour.do(scrape)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f85fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
